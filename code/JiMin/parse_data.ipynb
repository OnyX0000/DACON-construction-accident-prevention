{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"text\": \"철탑공사를 수행하는 사업주는 착공에서 준공까지 근로자의\n",
    "\t   유해-위험방지에 필요한 산업안전보건법 등의 관련 법규를 검토하여 \n",
    "\t\t 공사 착공 이전에 안전작업 계획을 수립하고 시행하여야 한다.\",\n",
    "  \"metadata\": {\n",
    "    \"title\": \"가공송전선로 철탑 심형기초공사 안전보건작업 지침\",\n",
    "    \"공사종류\": \"토목 / 기타 / 부지조성\"\n",
    "    \"공종\": \"토목 > 철골공사\",\n",
    "    \"장소\": \"기타 / 외부\"\n",
    "    \"section\": \"준비절차\"\n",
    "    \"page\": 2\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "# PDF 파일이 있는 디렉토리 경로 (현재 경로 기준)\n",
    "pdf_dir = Path(\"data/pdf\")  # 절대경로 필요하면 변경\n",
    "pdf_files = sorted(pdf_dir.glob(\"*.pdf\"))[:52]  # 최대 52개 파일만 선택\n",
    "\n",
    "# 문서 리스트\n",
    "documents = []\n",
    "\n",
    "# 메타데이터 변환 함수\n",
    "def custom_metadata(pdf_path: str, metadata: dict, page_number: int, section: str) -> dict:\n",
    "    return {\n",
    "        \"title\": metadata.get(\"Title\", \"Unknown\"),  # 제목 추가\n",
    "        \"section\": section,  # 섹션명 추가 (별도 추출 필요)\n",
    "        \"page\": page_number  # 페이지 번호\n",
    "    }\n",
    "\n",
    "# PDF 파일을 하나씩 처리\n",
    "for pdf_path in pdf_files:\n",
    "    with pdfplumber.open(str(pdf_path)) as pdf:  # Path 객체를 str로 변환\n",
    "        pdf_metadata = pdf.metadata or {}  # 메타데이터가 없으면 빈 딕셔너리\n",
    "\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:  # 빈 페이지 제외\n",
    "                # 섹션명 추출 로직 필요 (예: 첫 번째 큰 제목을 섹션으로 인식)\n",
    "                section = \"\"  # 섹션 추출 기능 추가 가능\n",
    "                metadata = custom_metadata(pdf_path, pdf_metadata, i + 1, section)\n",
    "                documents.append({\"text\": text.strip(), \"metadata\": metadata})\n",
    "\n",
    "# 결과 확인\n",
    "for doc in documents[:2]:  # 첫 두 개만 출력\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='KOSHA GUIDE\n",
      "C - 80 - 2013\n",
      "강아치교(벤트공법) 안전보건작업지침\n",
      "2013. 8.\n",
      "한국산업안전보건공단' metadata={'source': '강아치교(벤트공법) 안전보건작업지침.pdf', 'title': '강아치교(벤트공법) 안전보건작업지침', 'section': '', 'page': 1}\n",
      "page_content='안전보건기술지침의 개요\n",
      "○ 작성자 : 한국안전학회 최명기\n",
      "○ 제·개정 경과\n",
      "- 2013년 8월 건설안전분야 제정위원회 심의(제정)\n",
      "○ 관련규격 및 자료\n",
      "- 최신 교량공학(동명사, 황학주)\n",
      "- 도로교 표준시방서(대한토목학회)\n",
      "- 교량공사(강아치교) 시공자료\n",
      "- 교량공사 안전점검 Check Lisk : 한국산업안전보건공단 건설안전기술자료\n",
      "- 강교 설치 작업 안전 : 건설분야 교육자료 미디어개발 2009 456 1335\n",
      "○ 관련법규․규칙․고시 등\n",
      "- 산업안전보건기준에 관한 규칙 제136조～제150조, 제163조～제170조\n",
      "○ 기술지침의 적용 및 문의\n",
      "이 기술지침에 대한 의견 또는 문의는 한국산업안전보건공단 홈 페이지\n",
      "안전보건기술지침 소관 분야별 문의처 안내를 참고하시기 바랍니다.\n",
      "공표일자 : 2013년 10월 2일\n",
      "제 정 자 : 한국산업안전보건공단 이사장' metadata={'source': '강아치교(벤트공법) 안전보건작업지침.pdf', 'title': '강아치교(벤트공법) 안전보건작업지침', 'section': '', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "# PDF 파일이 있는 디렉토리 경로\n",
    "pdf_path = \"../../data/pdf/강아치교(벤트공법) 안전보건작업지침.pdf\"\n",
    "\n",
    "# 문서 리스트\n",
    "documents = []\n",
    "\n",
    "# 메타데이터 변환 함수\n",
    "def custom_metadata(pdf_path: str, page_number: int) -> dict:\n",
    "    return {\n",
    "        \"source\": os.path.basename(pdf_path),  # 파일 이름 저장\n",
    "        \"title\": \"강아치교(벤트공법) 안전보건작업지침\",  # 문서 제목\n",
    "        \"section\": \"\",  # 섹션명 (추후 자동 추출 가능)\n",
    "        \"page\": page_number  # 페이지 번호\n",
    "    }\n",
    "\n",
    "# PDF 파일 처리\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            metadata = custom_metadata(pdf_path, i + 1)\n",
    "            documents.append(Document(page_content=text.strip(), metadata=metadata))\n",
    "\n",
    "# 결과 확인\n",
    "for doc in documents[:2]:  # 첫 두 개만 출력\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Using cached unstructured-0.17.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: filetype in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured) (5.3.1)\n",
      "Collecting nltk (from unstructured)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured) (4.13.3)\n",
      "Collecting emoji (from unstructured)\n",
      "  Using cached emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Using cached python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured) (3.12.1)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from unstructured) (4.12.2)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.31.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from unstructured) (6.1.1)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from nltk->unstructured) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from requests->unstructured) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from requests->unstructured) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from requests->unstructured) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from requests->unstructured) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured-client->unstructured) (44.0.2)\n",
      "Collecting eval-type-backport>=0.2.0 (from unstructured-client->unstructured)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured-client->unstructured) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured-client->unstructured) (2.10.6)\n",
      "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
      "  Using cached pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Collecting typing-inspection>=0.4.0 (from unstructured-client->unstructured)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from pydantic>=2.10.3->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from pydantic>=2.10.3->unstructured-client->unstructured) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\miniconda3\\envs\\conda311\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Downloading unstructured-0.17.0-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 590.6/590.6 kB 21.9 MB/s eta 0:00:00\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading unstructured_client-0.31.1-py3-none-any.whl (166 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993313 sha256=29a471cee75b2d87916ddc4502bc859891d6092ed0244424a85f436685d39df5\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: wrapt, typing-inspection, python-magic, python-iso639, pypdf, olefile, langdetect, html5lib, eval-type-backport, emoji, chardet, backoff, aiofiles, python-oxmsg, nltk, unstructured-client, unstructured\n",
      "Successfully installed aiofiles-24.1.0 backoff-2.2.1 chardet-5.2.0 emoji-2.14.1 eval-type-backport-0.2.2 html5lib-1.1 langdetect-1.0.9 nltk-3.9.1 olefile-0.47 pypdf-5.4.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 typing-inspection-0.4.0 unstructured-0.17.0 unstructured-client-0.31.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/pdf\\\\F.C.M 교량공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\I.L.M 교량공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\PCT거더 교량공사 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\가공송전선로 철탑 심형기초공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\가설계단 설치 및 사용 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\가설구조물의 설계변경 요청 내용, 절차 등에 관한 작성지침.pdf',\n",
       " '../../data/pdf\\\\강관비계 안전작업지침.pdf',\n",
       " '../../data/pdf\\\\강박스거더 교량공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\강아치교(벤트공법) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\갱폼(Gang form) 제작 및 사용안전 지침.pdf',\n",
       " '../../data/pdf\\\\건설공사 굴착면 안전기울기 기준에 관한 기술지침.pdf',\n",
       " '../../data/pdf\\\\건설공사 돌관작업 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\건설공사 안전보건 설계 지침.pdf',\n",
       " '../../data/pdf\\\\건설공사의 고소작업대 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\건설기계 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\건설현장 용접용단 안전보건작업 기술지침.pdf',\n",
       " '../../data/pdf\\\\건설현장의 중량물 취급 작업계획서(이동식크레인) 작성지침.pdf',\n",
       " '../../data/pdf\\\\건축물의 석공사(내외장) 안전보건작업 기술지침.pdf',\n",
       " '../../data/pdf\\\\경량철골 천장공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\곤돌라(Gondola) 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\관로매설공사 안전보건작업 기술지침.pdf',\n",
       " '../../data/pdf\\\\관로매설공사(유압식 추진공법) 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\교량 슬래브거푸집 해체용 작업대차 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\교량공사(P.S.M공법) 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\교량공사(라멘교) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\교량공사의 이동식 비계공법(MSS) 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\굴착공사 계측관리 기술지침.pdf',\n",
       " '../../data/pdf\\\\굴착공사 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\굴착기 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\금속 커튼월(Curtain wall) 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\기성 콘크리트 파일 항타 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\낙하물 방지망 설치 지침.pdf',\n",
       " '../../data/pdf\\\\낙하물 방호선반 설치 지침.pdf',\n",
       " '../../data/pdf\\\\내장공사의 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\냉동냉장 물류창고 단열공사 화재예방 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\단순 슬래브 콘크리트 타설 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\덤프트럭 및 화물자동차 안전작업지침.pdf',\n",
       " '../../data/pdf\\\\리모델링 안전보건작업 기술지침.pdf',\n",
       " '../../data/pdf\\\\미장공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\밀폐공간의 방수공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\발파공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\블록식 보강토 옹벽 공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\사장교 교량공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\소규모 철근콘크리트 교량공사 거푸집 동바리 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\수상 바지(Barge)선 이용 건설공사 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\수직보호망 설치 지침.pdf',\n",
       " '../../data/pdf\\\\수직형 추락방망 설치 기술지침.pdf',\n",
       " '../../data/pdf\\\\슬립폼(Slip form) 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\시스템 동바리 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\시스템 비계 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\시스템폼(RCS폼,ACS폼 중심) 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\시트(Sheet)방수 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\아스팔트콘크리트 포장공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\안전대 사용지침.pdf',\n",
       " '../../data/pdf\\\\야간 건설공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\옹벽(콘크리트 옹벽)공사의 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\우물통기초 안전보건 작업지침.pdf',\n",
       " '../../data/pdf\\\\이동식 비계 설치 및 사용안전 기술지침.pdf',\n",
       " '../../data/pdf\\\\이동식 크레인 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\이동식 크레인 양중작업의 안정성 검토 지침.pdf',\n",
       " '../../data/pdf\\\\작업발판 설치 및 사용안전 지침.pdf',\n",
       " '../../data/pdf\\\\작업의자형 달비계 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\재사용 가설기자재 성능기준에 관한 지침.pdf',\n",
       " '../../data/pdf\\\\조경공사(수목식재작업) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\조적공사 안전보건작업 기술지침.pdf',\n",
       " '../../data/pdf\\\\중소규모 건설업체 본사의 안전보건관리에 관한 지침.pdf',\n",
       " '../../data/pdf\\\\중소규모 관로공사의 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\지붕공사 안전보건작업 기술지침.pdf',\n",
       " '../../data/pdf\\\\지하매설물 굴착공사 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\철골공사 무지보 거푸집동바리(데크플레이트 공법)안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\철골공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\철탑공사 안전보건기술지침.pdf',\n",
       " '../../data/pdf\\\\초고층 건축물공사(일반사항) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\초고층 건축물공사(화재예방) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\추락방호망 설치 지침.pdf',\n",
       " '../../data/pdf\\\\취약시기 건설현장 안전작업지침.pdf',\n",
       " '../../data/pdf\\\\콘크리트공사의 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\타워크레인 설치, 조립, 해체 작업계획서 작성지침.pdf',\n",
       " '../../data/pdf\\\\타일(Tile) 공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\탑다운(Top down) 공법 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\터널공사(NATM공법) 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\터널공사(NTR공법) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\터널공사(Shield-T.B.M공법) 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\터널공사(침매공법) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\터널공사(프론트잭킹) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\트러스거더 교량공사 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\트럭 탑재형 크레인(Cago crane) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\파이프 서포트 동바리 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\프리스트레스트 콘크리트(PSC) 교량공사 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\프리캐스트 콘크리트 건축구조물 조립 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\항타기, 항발기 사용 작업계획서 작성지침.pdf',\n",
       " '../../data/pdf\\\\해상 RCD 현장타설 말뚝공사(현수교, 사장교) 안전작업 지침.pdf',\n",
       " '../../data/pdf\\\\해체공사 안전보건작업 기술지침.pdf',\n",
       " '../../data/pdf\\\\현수교 교량공사 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\현수교 주탑시공 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\화학플랜트 개보수 공사 안전보건작업 기술지침.pdf',\n",
       " '../../data/pdf\\\\흙막이공사 (SCW 공법) 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\흙막이공사(C.I.P공법) 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\흙막이공사(Earth Anchor 공법) 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\흙막이공사(Soil Nailing 공법) 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\흙막이공사(강널말뚝, Sheet Pile)의 안전보건작업지침.pdf',\n",
       " '../../data/pdf\\\\흙막이공사(띠장긴장공법, Prestressed Wale Method) 안전보건 작업지침.pdf',\n",
       " '../../data/pdf\\\\흙막이공사(엄지말뚝 공법) 안전보건작업 지침.pdf',\n",
       " '../../data/pdf\\\\흙막이공사(지하연속벽) 안전보건작업 지침.pdf']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "files = glob(os.path.join('../../data/pdf', '*.pdf'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1816"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "loader = DirectoryLoader(path='../../data/pdf', glob='*.pdf', loader_cls=PyPDFLoader)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='2. 적용범위' metadata={'source': 'F.C.M 교량공사 안전보건작업 지침.pdf', 'title': '2. 적용범위'}\n",
      "page_content='3. 용어의 정의' metadata={'source': 'F.C.M 교량공사 안전보건작업 지침.pdf', 'title': '3. 용어의 정의'}\n",
      "page_content='2. 적용범위' metadata={'source': 'I.L.M 교량공사 안전보건작업 지침.pdf', 'title': '2. 적용범위'}\n",
      "page_content='3. 용어의 정의' metadata={'source': 'I.L.M 교량공사 안전보건작업 지침.pdf', 'title': '3. 용어의 정의'}\n",
      "page_content='9. 기타 안전조치 사항' metadata={'source': 'I.L.M 교량공사 안전보건작업 지침.pdf', 'title': '9. 기타 안전조치 사항'}\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "# PDF 파일이 있는 디렉토리 경로\n",
    "pdf_dir = Path(\"../../data/pdf/\")\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "# 제목 패턴 정의 (정규 표현식)\n",
    "title_patterns = [\n",
    "    r\"^\\d+\\.\\s*[가-힣A-Za-z\\s]+\",  # \"1. 제목\" 형태\n",
    "    r\"^○\\s*[가-힣A-Za-z\\s]+\",  # \"○ 제목\" 형태\n",
    "    r\"^\\[[가-힣A-Za-z\\s]+\\]\",  # \"[제목]\" 형태\n",
    "    r\"^\\<\\<[가-힣A-Za-z\\s]+\\>\\>\",  # \"<<제목>>\" 형태\n",
    "    r\"^\\【[가-힣A-Za-z\\s]+\\】\"  # \"【제목】\" 형태\n",
    "]\n",
    "\n",
    "# 유지할 섹션 키워드 (필터링 기준)\n",
    "include_sections = [\n",
    "    \"목적\", \"적용범위\", \"용어의 정의\", \"사고 원인 분석\",\n",
    "    \"사고처리 및 응급조치\", \"안전조치 사항\", \"예방 대책\", \"작업별 안전 조치\",\n",
    "    \"시공 시 안전작업\", \"제작 및 설치 순서\"\n",
    "]\n",
    "\n",
    "# 제외할 섹션 키워드\n",
    "exclude_sections = [\n",
    "    \"작성자\", \"기술지침의 적용 및 문의\", \"관련법규\", \"관련 규격\", \"출처\"\n",
    "]\n",
    "\n",
    "# 문서 리스트\n",
    "documents = []\n",
    "\n",
    "# PDF 파일을 순회하면서 제목 패턴 추출\n",
    "def extract_titles_from_pdf(pdf_path):\n",
    "    extracted_sections = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    for line in text.split(\"\\n\"):\n",
    "                        line = line.strip()\n",
    "                        for pattern in title_patterns:\n",
    "                            if re.match(pattern, line):\n",
    "                                extracted_sections.append(line)\n",
    "        return extracted_sections\n",
    "    except Exception as e:\n",
    "        print(f\"파일 처리 오류: {pdf_path}, 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "# PDF에서 제목 패턴 추출 후 필터링 적용\n",
    "for pdf_path in pdf_files:\n",
    "    sections = extract_titles_from_pdf(pdf_path)\n",
    "    for section in sections:\n",
    "        # 유지할 섹션인지 확인\n",
    "        if any(inc in section for inc in include_sections) and not any(exc in section for exc in exclude_sections):\n",
    "            documents.append(Document(\n",
    "                page_content=section,\n",
    "                metadata={\n",
    "                    \"source\": os.path.basename(pdf_path),\n",
    "                    \"title\": section\n",
    "                }\n",
    "            ))\n",
    "\n",
    "# 결과 확인\n",
    "for doc in documents[:5]:  # 처음 5개만 출력\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 데이터가 JSON 파일로 저장되었습니다: ./json/extracted_data.json\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "# PDF 파일이 있는 디렉토리 경로\n",
    "pdf_dir = Path(\"../../data/pdf/\")\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "# 제목 패턴 정의 (정규 표현식)\n",
    "title_patterns = [\n",
    "    r\"^\\d+\\.\\s*[가-힣A-Za-z\\s]+\",  # \"1. 제목\" 형태\n",
    "    r\"^○\\s*[가-힣A-Za-z\\s]+\",  # \"○ 제목\" 형태\n",
    "    r\"^\\[[가-힣A-Za-z\\s]+\\]\",  # \"[제목]\" 형태\n",
    "    r\"^\\<\\<[가-힣A-Za-z\\s]+\\>\\>\",  # \"<<제목>>\" 형태\n",
    "    r\"^\\【[가-힣A-Za-z\\s]+\\】\"  # \"【제목】\" 형태\n",
    "]\n",
    "\n",
    "# 유지할 섹션 키워드 (필터링 기준)\n",
    "include_sections = [\n",
    "    \"목적\", \"적용범위\", \"용어의 정의\", \"사고 원인 분석\",\n",
    "    \"사고처리 및 응급조치\", \"안전조치 사항\", \"예방 대책\", \"작업별 안전 조치\",\n",
    "    \"시공 시 안전작업\", \"제작 및 설치 순서\"\n",
    "]\n",
    "\n",
    "# 제외할 섹션 키워드\n",
    "exclude_sections = [\n",
    "    \"작성자\", \"기술지침의 적용 및 문의\", \"관련법규\", \"관련 규격\", \"출처\"\n",
    "]\n",
    "\n",
    "# 문서 리스트\n",
    "documents = []\n",
    "\n",
    "# PDF 파일을 순회하면서 제목 패턴 추출 및 본문 데이터 가져오기\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    extracted_data = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    for i, line in enumerate(lines):\n",
    "                        line = line.strip()\n",
    "                        for pattern in title_patterns:\n",
    "                            if re.match(pattern, line):\n",
    "                                if any(inc in line for inc in include_sections) and not any(exc in line for exc in exclude_sections):\n",
    "                                    section_text = \" \".join(lines[i+1:i+5])  # 제목 이후 일부 내용 가져오기\n",
    "                                    extracted_data.append({\n",
    "                                        \"text\": section_text,\n",
    "                                        \"metadata\": {\n",
    "                                            \"title\": os.path.basename(pdf_path),\n",
    "                                            \"section\": line,\n",
    "                                            \"page\": page_num\n",
    "                                        }\n",
    "                                    })\n",
    "        return extracted_data\n",
    "    except Exception as e:\n",
    "        print(f\"파일 처리 오류: {pdf_path}, 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "# PDF에서 데이터 추출 후 JSON 형식으로 저장\n",
    "for pdf_path in pdf_files:\n",
    "    sections = extract_data_from_pdf(pdf_path)\n",
    "    documents.extend(sections)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "output_json_path = \"./json/extracted_data.json\"\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"추출된 데이터가 JSON 파일로 저장되었습니다: {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '이 지침은 바닥으로부터 동바리를 사용하지 않고 교각위 주두부(Pier table)를 시공한 후 교각 양쪽의 교축방향으로 특수한 가설장비(F/T, Form traveller)를 이용해 좌․우로 하중의 균형을 맞추면서 세그먼트(Segment)의 콘크리트 타설, 프리스트레싱 도입을 순차적으로 반복하여 교량 상부 구조를 완성하는 현장타', 'metadata': {'title': 'F.C.M 교량공사 안전보건작업 지침.pdf', 'section': '2. 적용범위', 'page': 3}}, {'text': '(1) 이 지침에서 사용되는 용어의 정의는 다음과 같다. (가) “F.C.M 공법(Free Cantilever Method)”이라 함은 동바리 없이 기 시공되 어 있는 교각을 이용하여 교각의 좌ᐧ우로 하중의 균형을 맞추면서 이동 식 작업대차(Form traveller)나 이동식 가설 트러스(Moving gantry)를 이', 'metadata': {'title': 'F.C.M 교량공사 안전보건작업 지침.pdf', 'section': '3. 용어의 정의', 'page': 4}}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 로드\n",
    "json_path = \"./json/extracted_data.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "print(documents[:2])  # 첫 2개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 데이터가 JSON 파일로 저장되었습니다: ./json/extracted_data.json\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# PDF 파일이 있는 디렉토리 경로\n",
    "pdf_dir = Path(\"../../data/pdf/\")\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "# 제목 패턴 정의 (정규 표현식)\n",
    "title_patterns = [\n",
    "    r\"^\\d+\\.\\s*[가-힣A-Za-z\\s]+\",  # \"1. 제목\" 형태\n",
    "    r\"^○\\s*[가-힣A-Za-z\\s]+\",  # \"○ 제목\" 형태\n",
    "    r\"^\\[[가-힣A-Za-z\\s]+\\]\",  # \"[제목]\" 형태\n",
    "    r\"^\\<\\<[가-힣A-Za-z\\s]+\\>\\>\",  # \"<<제목>>\" 형태\n",
    "    r\"^\\【[가-힣A-Za-z\\s]+\\】\"  # \"【제목】\" 형태\n",
    "]\n",
    "\n",
    "# 유지할 섹션 키워드 (필터링 기준)\n",
    "include_sections = [\n",
    "    \"목적\", \"적용범위\", \"용어의 정의\", \"사고 원인 분석\",\n",
    "    \"사고처리 및 응급조치\", \"안전조치 사항\", \"예방 대책\", \"작업별 안전 조치\",\n",
    "    \"시공 시 안전작업\", \"제작 및 설치 순서\"\n",
    "]\n",
    "\n",
    "# 제외할 섹션 키워드\n",
    "exclude_sections = [\n",
    "    \"작성자\", \"기술지침의 적용 및 문의\", \"관련법규\", \"관련 규격\", \"출처\"\n",
    "]\n",
    "\n",
    "# 텍스트 청크 중첩 설정\n",
    "chunk_size = 512  # 한 청크에 포함할 최대 토큰 수\n",
    "chunk_overlap = 128  # Overlap 비율\n",
    "\n",
    "# 문서 리스트\n",
    "documents = []\n",
    "\n",
    "# 텍스트 스플리터 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# PDF 파일을 순회하면서 제목 패턴 추출 및 본문 데이터 가져오기\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    extracted_data = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            full_text = \"\"\n",
    "            metadata = []\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    current_section = None\n",
    "                    for line in lines:\n",
    "                        line = line.strip()\n",
    "                        if any(re.match(pattern, line) for pattern in title_patterns):\n",
    "                            if any(inc in line for inc in include_sections) and not any(exc in line for exc in exclude_sections):\n",
    "                                if current_section and full_text:\n",
    "                                    chunks = text_splitter.split_text(full_text)\n",
    "                                    for chunk in chunks:\n",
    "                                        extracted_data.append({\n",
    "                                            \"text\": chunk,\n",
    "                                            \"metadata\": {\n",
    "                                                \"title\": os.path.basename(pdf_path),\n",
    "                                                \"section\": current_section,\n",
    "                                                \"page\": page_num\n",
    "                                            }\n",
    "                                        })\n",
    "                                full_text = \"\"  # 새 섹션 시작 시 초기화\n",
    "                                current_section = line  # 현재 섹션 업데이트\n",
    "                        else:\n",
    "                            full_text += \" \" + line  # 본문 텍스트 추가\n",
    "            \n",
    "            # 마지막 섹션도 저장\n",
    "            if current_section and full_text:\n",
    "                chunks = text_splitter.split_text(full_text)\n",
    "                for chunk in chunks:\n",
    "                    extracted_data.append({\n",
    "                        \"text\": chunk,\n",
    "                        \"metadata\": {\n",
    "                            \"title\": os.path.basename(pdf_path),\n",
    "                            \"section\": current_section,\n",
    "                            \"page\": page_num\n",
    "                        }\n",
    "                    })\n",
    "        return extracted_data\n",
    "    except Exception as e:\n",
    "        print(f\"파일 처리 오류: {pdf_path}, 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "# PDF에서 데이터 추출 후 JSON 형식으로 저장\n",
    "for pdf_path in pdf_files:\n",
    "    sections = extract_data_from_pdf(pdf_path)\n",
    "    documents.extend(sections)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "output_json_path = \"./json/extracted_data.json\"\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"추출된 데이터가 JSON 파일로 저장되었습니다: {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '(1) 작업 시작 전에 안전담당자를 지정하여 작업을 지휘하도록 하여야 한다. (2) 근로자의 건강상태를 작업 전에 확인하여 작업배치 적정여부를 결정하여야 한다. (3) 공사 중 인양하중을 고려하여 크레인 용량을 검토하되 외국제품의 경우 국내의 정격하중 기준이 다를 수 있으므로 유의하여야 한다. (4) 크레인 작업에는 신호수를 배치하고 운전원과 신호수간 신호를 통일하여 운영하여야 한다. (5) 크레인에는 낙뢰에 대비하여 추가적으로 피뢰설비를 설치하여야 한다. (6) 크레인 운전원은 다음의 안전수칙을 준수하여야 한다. (가) 크레인 사용은 지정된 운전원이 하여야 한다. (나) 크레인 운전원의 개인보호구 및 장비는 무선조종기 조작장치와 간섭 되지 않도록 유의하여야 한다. (다) 크레인의 안전장치를 임의로 제거 또는 변경해서는 안된다. (라) 크레인 사용 시 급운전, 급정지, 급강하, 급상승을 하여서는 안된다. (마) 크레인의 정격 인양하중을 준수한다. (7) 안전모, 안전대 등 근로자의 개인보호구를 점검하고', 'metadata': {'title': 'I.L.M 교량공사 안전보건작업 지침.pdf', 'section': '9. 기타 안전조치 사항', 'page': 18}}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 로드\n",
    "json_path = \"./json/extracted_data.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "print(documents[:1])  # 첫 2개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\conda311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 데이터가 JSON 파일로 저장되었으며, 벡터 DB에 저장되었습니다: ./json/extracted_data2.json\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# PDF 파일이 있는 디렉토리 경로\n",
    "pdf_dir = Path(\"../../data/pdf/\")\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "# 제목 패턴 정의 (정규 표현식)\n",
    "title_patterns = [\n",
    "    r\"^\\d+\\.\\s*[가-힣A-Za-z\\s]+\",  # \"1. 제목\" 형태\n",
    "    r\"^○\\s*[가-힣A-Za-z\\s]+\",  # \"○ 제목\" 형태\n",
    "    r\"^\\[[가-힣A-Za-z\\s]+\\]\",  # \"[제목]\" 형태\n",
    "    r\"^\\<\\<[가-힣A-Za-z\\s]+\\>\\>\",  # \"<<제목>>\" 형태\n",
    "    r\"^\\【[가-힣A-Za-z\\s]+\\】\"  # \"【제목】\" 형태\n",
    "]\n",
    "\n",
    "# 유지할 섹션 키워드 (필터링 기준)\n",
    "include_sections = [\n",
    "    \"목적\", \"적용범위\", \"용어의 정의\", \"사고 원인 분석\",\n",
    "    \"사고처리 및 응급조치\", \"안전조치 사항\", \"예방 대책\", \"작업별 안전 조치\",\n",
    "    \"시공 시 안전작업\", \"제작 및 설치 순서\"\n",
    "]\n",
    "\n",
    "# 제외할 섹션 키워드\n",
    "exclude_sections = [\n",
    "    \"작성자\", \"기술지침의 적용 및 문의\", \"관련법규\", \"관련 규격\", \"출처\"\n",
    "]\n",
    "\n",
    "# 텍스트 청크 중첩 설정\n",
    "chunk_size = 512  # 한 청크에 포함할 최대 토큰 수\n",
    "chunk_overlap = 128  # Overlap 비율\n",
    "\n",
    "# 문서 리스트\n",
    "documents = []\n",
    "\n",
    "# 텍스트 스플리터 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# SBERT 한국어 모델(S-BERT) 불러오기\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# ChromaDB 클라이언트 생성\n",
    "chroma_client = chromadb.PersistentClient(path=\"mnt/data/chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"safety_docs\")\n",
    "\n",
    "# PDF 파일을 순회하면서 제목 패턴 추출 및 본문 데이터 가져오기\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    extracted_data = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            full_text = \"\"\n",
    "            metadata = []\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    current_section = None\n",
    "                    for line in lines:\n",
    "                        line = line.strip()\n",
    "                        if any(re.match(pattern, line) for pattern in title_patterns):\n",
    "                            if any(inc in line for inc in include_sections) and not any(exc in line for exc in exclude_sections):\n",
    "                                if current_section and full_text:\n",
    "                                    chunks = text_splitter.split_text(full_text)\n",
    "                                    for chunk in chunks:\n",
    "                                        embedding = model.encode(chunk).tolist()\n",
    "                                        collection.add(\n",
    "                                            ids=[str(len(extracted_data))],\n",
    "                                            embeddings=[embedding],\n",
    "                                            metadatas=[{\n",
    "                                                \"title\": os.path.basename(pdf_path),\n",
    "                                                \"section\": current_section,\n",
    "                                                \"page\": page_num\n",
    "                                            }]\n",
    "                                        )\n",
    "                                        extracted_data.append({\n",
    "                                            \"text\": chunk,\n",
    "                                            \"metadata\": {\n",
    "                                                \"title\": os.path.basename(pdf_path),\n",
    "                                                \"section\": current_section,\n",
    "                                                \"page\": page_num\n",
    "                                            }\n",
    "                                        })\n",
    "                                full_text = \"\"  # 새 섹션 시작 시 초기화\n",
    "                                current_section = line  # 현재 섹션 업데이트\n",
    "                        else:\n",
    "                            full_text += \" \" + line  # 본문 텍스트 추가\n",
    "            \n",
    "            # 마지막 섹션도 저장\n",
    "            if current_section and full_text:\n",
    "                chunks = text_splitter.split_text(full_text)\n",
    "                for chunk in chunks:\n",
    "                    embedding = model.encode(chunk).tolist()\n",
    "                    collection.add(\n",
    "                        ids=[str(len(extracted_data))],\n",
    "                        embeddings=[embedding],\n",
    "                        metadatas=[{\n",
    "                            \"title\": os.path.basename(pdf_path),\n",
    "                            \"section\": current_section,\n",
    "                            \"page\": page_num\n",
    "                        }]\n",
    "                    )\n",
    "                    extracted_data.append({\n",
    "                        \"text\": chunk,\n",
    "                        \"metadata\": {\n",
    "                            \"title\": os.path.basename(pdf_path),\n",
    "                            \"section\": current_section,\n",
    "                            \"page\": page_num\n",
    "                        }\n",
    "                    })\n",
    "        return extracted_data\n",
    "    except Exception as e:\n",
    "        print(f\"파일 처리 오류: {pdf_path}, 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "# PDF에서 데이터 추출 후 JSON 형식으로 저장\n",
    "for pdf_path in pdf_files:\n",
    "    sections = extract_data_from_pdf(pdf_path)\n",
    "    documents.extend(sections)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "output_json_path = \"./json/extracted_data2.json\"\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"추출된 데이터가 JSON 파일로 저장되었으며, 벡터 DB에 저장되었습니다: {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 사용 설정 (GPU가 있으면 \"cuda\", 없으면 \"cpu\")\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 기존 데이터를 유지하면서 train.csv 데이터 추가 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# CSV 파일 로드\n",
    "train_csv_path = \"../../data/train.csv\"\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "# ChromaDB 클라이언트 생성 (기존 데이터 유지)\n",
    "chroma_client = chromadb.PersistentClient(path=\"mnt/data/chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"safety_incidents\")\n",
    "\n",
    "# SBERT 모델 로드\n",
    "# model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# GPU 사용 설정 (GPU가 있으면 \"cuda\", 없으면 \"cpu\")\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", device=\"cuda\")\n",
    "\n",
    "\n",
    "# 기존 데이터 개수 확인\n",
    "existing_data_count = len(collection.get()[\"ids\"]) if collection.get() else 0\n",
    "\n",
    "# 벡터화 및 DB 저장 (중복 방지)\n",
    "for idx, row in train_df.iterrows():\n",
    "    unique_id = f\"train_{existing_data_count + idx}\"  # 기존 데이터 개수를 고려한 고유 ID\n",
    "\n",
    "    incident_text = str(row[\"사고원인\"])\n",
    "    metadata = {\n",
    "        \"ID\": row[\"ID\"],\n",
    "        \"공사종류\": row[\"공사종류\"],\n",
    "        \"공종\": row[\"공종\"],\n",
    "        \"사고객체\": row[\"사고객체\"],\n",
    "        \"작업프로세스\": row[\"작업프로세스\"],\n",
    "        \"장소\": row[\"장소\"],\n",
    "        \"부위\": row[\"부위\"],\n",
    "        \"재발방지대책\": row[\"재발방지대책 및 향후조치계획\"],\n",
    "    }\n",
    "\n",
    "    # 벡터화\n",
    "    embedding = model.encode(incident_text).tolist()\n",
    "\n",
    "    # ChromaDB에 추가 (중복 없이)\n",
    "    collection.add(\n",
    "        ids=[unique_id],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[metadata]\n",
    "    )\n",
    "\n",
    "print(\"✅ 기존 데이터를 유지하면서 train.csv 데이터 추가 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 유사 사례 1:\n",
      "사고원인: None\n",
      "공사종류: 건축 / 건축물 / 의료시설\n",
      "공종: 건축 > 철근콘크리트공사\n",
      "작업프로세스: 해체작업\n",
      "재발방지대책: 해당 작업팀의 사고 사례 전파 및 재해 방지 교육 실시와 위험성 평가 내용 추가에 의한 지속 관리.\n",
      "\n",
      "🔍 유사 사례 2:\n",
      "사고원인: None\n",
      "공사종류: 토목 / 항만 / 방파제\n",
      "공종: 토목 > 항만공사\n",
      "작업프로세스: 해체작업\n",
      "재발방지대책: 거푸집 및 고정용 파이프 서포트 탈락 방지용 고리 설치와 고위험 작업에 따른 관리감독자 및 숙련공 배치 여부 확인.\n",
      "\n",
      "🔍 유사 사례 3:\n",
      "사고원인: None\n",
      "공사종류: 건축 / 건축물 / 교육연구시설\n",
      "공종: 건축 > 가설공사\n",
      "작업프로세스: 설치작업\n",
      "재발방지대책: 안전관리자의 주도로 현장 작업 전 교육 추가 진행, 고사 작업 등 위험작업 시 안전작업허가서 발행, 위험 작업 시 안전관리자 입회, 안전관리자 현장 점검 강화와 시공사가 최초 발주처를 신탁사로 기재하고 인허가기관에 보고함에 따른 사고조사 입력 지연에 대한 재발 방지 대책 준수 요청.\n",
      "\n",
      "✅ 유사 사고 검색 완료!\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 로드\n",
    "test_csv_path = \"../../data/test.csv\"\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# 검색할 사고원인 가져오기\n",
    "query_text = test_df.iloc[0][\"사고원인\"]  # 첫 번째 테스트 데이터 사용\n",
    "query_embedding = model.encode(query_text).tolist()\n",
    "\n",
    "# 유사 사례 검색\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3  # 상위 3개 검색\n",
    ")\n",
    "\n",
    "# 검색 결과 출력\n",
    "for i, result in enumerate(results[\"documents\"][0]):\n",
    "    print(f\"\\n🔍 유사 사례 {i+1}:\")\n",
    "    print(f\"사고원인: {result}\")\n",
    "    print(f\"공사종류: {results['metadatas'][0][i]['공사종류']}\")\n",
    "    print(f\"공종: {results['metadatas'][0][i]['공종']}\")\n",
    "    print(f\"작업프로세스: {results['metadatas'][0][i]['작업프로세스']}\")\n",
    "    print(f\"재발방지대책: {results['metadatas'][0][i]['재발방지대책']}\")\n",
    "\n",
    "print(\"\\n✅ 유사 사고 검색 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\conda311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU 사용, 배치 저장 적용, 사고원인 None 방지, 데이터 손실 없이 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "# CSV 파일 로드\n",
    "train_csv_path = \"../../data/train.csv\"\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "# NaN 값 제거 및 데이터 정리\n",
    "train_df.fillna(\"정보 없음\", inplace=True)\n",
    "\n",
    "# ChromaDB 클라이언트 생성 (기존 데이터 유지)\n",
    "chroma_client = chromadb.PersistentClient(path=\"mnt/data/chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"safety_incidents\")\n",
    "\n",
    "# GPU 사용 설정 (GPU가 있으면 \"cuda\", 없으면 \"cpu\")\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", device=\"cuda\")\n",
    "\n",
    "# 배치 설정\n",
    "batch_size = 50\n",
    "batch_embeddings = []\n",
    "batch_metadata = []\n",
    "batch_ids = []\n",
    "\n",
    "# 기존 데이터 개수 확인 (중복 방지용 ID 생성)\n",
    "existing_data_count = len(collection.get()[\"ids\"]) if collection.get() else 0\n",
    "\n",
    "# 벡터화 및 DB 저장 (배치 적용 & None 방지)\n",
    "for idx, row in train_df.iterrows():\n",
    "    unique_id = f\"train_{existing_data_count + idx}\"  # 고유 ID 생성\n",
    "    incident_text = str(row[\"사고원인\"]) if pd.notna(row[\"사고원인\"]) else \"사고원인 없음\"\n",
    "    \n",
    "    metadata = {\n",
    "        \"ID\": row[\"ID\"],\n",
    "        \"공사종류\": row[\"공사종류\"],\n",
    "        \"공종\": row[\"공종\"],\n",
    "        \"사고객체\": row[\"사고객체\"],\n",
    "        \"작업프로세스\": row[\"작업프로세스\"],\n",
    "        \"장소\": row[\"장소\"],\n",
    "        \"부위\": row[\"부위\"],\n",
    "        \"재발방지대책\": str(row[\"재발방지대책 및 향후조치계획\"]) if pd.notna(row[\"재발방지대책 및 향후조치계획\"]) else \"대책 없음\"\n",
    "    }\n",
    "    \n",
    "    # 벡터화\n",
    "    embedding = model.encode(incident_text).tolist()\n",
    "    \n",
    "    # 배치에 추가\n",
    "    batch_embeddings.append(embedding)\n",
    "    batch_metadata.append(metadata)\n",
    "    batch_ids.append(unique_id)\n",
    "    \n",
    "    # 배치 크기만큼 모이면 저장\n",
    "    if len(batch_embeddings) >= batch_size:\n",
    "        collection.add(\n",
    "            ids=batch_ids,\n",
    "            embeddings=batch_embeddings,\n",
    "            metadatas=batch_metadata\n",
    "        )\n",
    "        batch_embeddings = []\n",
    "        batch_metadata = []\n",
    "        batch_ids = []\n",
    "\n",
    "# 남은 데이터 저장\n",
    "if batch_embeddings:\n",
    "    collection.add(\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings,\n",
    "        metadatas=batch_metadata\n",
    "    )\n",
    "\n",
    "print(\" GPU 사용, 배치 저장 적용, 사고원인 None 방지, 데이터 손실 없이 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 유사 사례 1:\n",
      "사고원인: None\n",
      "공사종류: 건축 / 건축물 / 의료시설\n",
      "공종: 건축 > 철근콘크리트공사\n",
      "작업프로세스: 해체작업\n",
      "재발방지대책: 해당 작업팀의 사고 사례 전파 및 재해 방지 교육 실시와 위험성 평가 내용 추가에 의한 지속 관리.\n",
      "\n",
      "🔍 유사 사례 2:\n",
      "사고원인: None\n",
      "공사종류: 건축 / 건축물 / 의료시설\n",
      "공종: 건축 > 철근콘크리트공사\n",
      "작업프로세스: 해체작업\n",
      "재발방지대책: 해당 작업팀의 사고 사례 전파 및 재해 방지 교육 실시와 위험성 평가 내용 추가에 의한 지속 관리.\n",
      "\n",
      "🔍 유사 사례 3:\n",
      "사고원인: None\n",
      "공사종류: 토목 / 항만 / 방파제\n",
      "공종: 토목 > 항만공사\n",
      "작업프로세스: 해체작업\n",
      "재발방지대책: 거푸집 및 고정용 파이프 서포트 탈락 방지용 고리 설치와 고위험 작업에 따른 관리감독자 및 숙련공 배치 여부 확인.\n",
      "\n",
      "✅ 유사 사고 검색 완료!\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 로드\n",
    "test_csv_path = \"../../data/test.csv\"\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# 검색할 사고원인 가져오기\n",
    "query_text = test_df.iloc[0][\"사고원인\"]  # 첫 번째 테스트 데이터 사용\n",
    "query_embedding = model.encode(query_text).tolist()\n",
    "\n",
    "# 유사 사례 검색\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3  # 상위 3개 검색\n",
    ")\n",
    "\n",
    "# 검색 결과 출력\n",
    "for i, result in enumerate(results[\"documents\"][0]):\n",
    "    print(f\"\\n🔍 유사 사례 {i+1}:\")\n",
    "    print(f\"사고원인: {result}\")\n",
    "    print(f\"공사종류: {results['metadatas'][0][i]['공사종류']}\")\n",
    "    print(f\"공종: {results['metadatas'][0][i]['공종']}\")\n",
    "    print(f\"작업프로세스: {results['metadatas'][0][i]['작업프로세스']}\")\n",
    "    print(f\"재발방지대책: {results['metadatas'][0][i]['재발방지대책']}\")\n",
    "\n",
    "print(\"\\n✅ 유사 사고 검색 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
